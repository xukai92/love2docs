import asyncio
import aiohttp
import json
import re
from pathlib import Path
from bs4 import BeautifulSoup
from typing import List, Dict
import requests
import click


class Love2DWikiScraper:
    def __init__(self, max_concurrent: int, base_url: str = "https://love2d.org/w"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api.php"
        self.max_concurrent = max_concurrent
        
    def get_all_love_pages(self, prefix: str = "love") -> List[Dict]:
        """Fetch all pages starting with the given prefix."""
        pages = []
        continue_token = None
        
        with requests.Session() as session:
            while True:
                params = {
                    "action": "query",
                    "list": "allpages",
                    "format": "json",
                    "aplimit": 500,
                    "apprefix": prefix
                }
                
                if continue_token:
                    params["apcontinue"] = continue_token
                    
                response = session.get(self.api_url, params=params)
                response.raise_for_status()
                data = response.json()
                
                if "query" in data and "allpages" in data["query"]:
                    pages.extend(data["query"]["allpages"])
                    
                if "continue" not in data:
                    break
                    
                continue_token = data["continue"]["apcontinue"]
            
        return pages
    
    async def get_page_content_async(self, session: aiohttp.ClientSession, title: str) -> tuple[str, str]:
        """Get the plain text content of a page asynchronously."""
        params = {
            "action": "query",
            "format": "json",
            "titles": title,
            "prop": "revisions",
            "rvprop": "content"
        }
        
        async with session.get(self.api_url, params=params) as response:
            response.raise_for_status()
            data = await response.json()
            
            if "query" in data and "pages" in data["query"]:
                page_data = list(data["query"]["pages"].values())[0]
                if "revisions" in page_data and page_data["revisions"]:
                    return title, page_data["revisions"][0].get("*", "")
                    
        return title, ""
    
    async def fetch_pages_batch(self, pages: List[Dict]) -> List[tuple[str, str]]:
        """Fetch multiple pages concurrently."""
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def fetch_with_semaphore(session: aiohttp.ClientSession, page: Dict) -> tuple[str, str]:
            async with semaphore:
                return await self.get_page_content_async(session, page["title"])
        
        async with aiohttp.ClientSession() as session:
            tasks = [fetch_with_semaphore(session, page) for page in pages]
            results = []
            
            for i, task in enumerate(asyncio.as_completed(tasks), 1):
                title, content = await task
                print(f"Processing {i}/{len(pages)}: {title}")
                results.append((title, content))
                
            return results
    
    def save_pages_to_file(self, pages: List[Dict], output_file: str = "love2d_wiki.txt"):
        """Save all page contents to a single text file."""
        output_path = Path(output_file)
        
        # Get all page contents asynchronously
        page_contents = asyncio.run(self.fetch_pages_batch(pages))
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("# Love2D Wiki Documentation\n")
            f.write("# Generated by love2docs\n\n")
            
            for title, content in page_contents:
                if content.strip():
                    f.write(f"## {title}\n\n")
                    f.write(content)
                    f.write("\n\n" + "="*80 + "\n\n")
                    
        print(f"Saved {len(pages)} pages to {output_path}")


@click.command()
@click.option('--concurrency', '-c', default=8, help='Number of concurrent requests', type=int)
@click.option('--output', '-o', default='love2d_docs.txt', help='Output file name')
@click.option('--prefix', '-p', default='love', help='Page prefix to filter')
def main(concurrency: int, output: str, prefix: str):
    """Download Love2D wiki documentation as plain text."""
    scraper = Love2DWikiScraper(concurrency)
    
    print(f"Fetching all Love2D wiki pages with prefix '{prefix}'...")
    pages = scraper.get_all_love_pages(prefix)
    
    # Filter to focus on main love.* API pages (English only)
    language_tags = ["(Español)", "(Français)", "(Nederlands)", "(Русский)", "(日本語)", "(한국어)", 
                     "(Português)", "(Deutsch)", "(Magyar)", "(Tiếng Việt)", "(Українська)",
                     "(Italiano)", "(Română)", "(Српски)", "(简体中文)", "(Slovenský)", "(Polski)",
                     "(Indonesia)", "(Česky)", "(Türkçe)", "(Svenska)", "(Dansk)", "(正體中文)"]
    
    # Additional tutorial/documentation pages to include
    additional_pages = [
        {"title": "Tutorial:Callback_Functions", "pageid": 0},
        {"title": "Getting_Started", "pageid": 0},
        {"title": "Building_LÖVE", "pageid": 0},
        {"title": "Game_Distribution", "pageid": 0},
        {"title": "Config_Files", "pageid": 0}
    ]
    
    love_api_pages = [p for p in pages if (p["title"].startswith("love.") or p["title"] == "love") 
                      and not any(lang in p["title"] for lang in language_tags)]
    
    # Add the additional pages
    love_api_pages.extend(additional_pages)
    
    print(f"Found {len(pages)} total pages, {len(love_api_pages)} love API pages")
    print(f"Using {concurrency} concurrent requests")
    
    # Save all love.* pages
    scraper.save_pages_to_file(love_api_pages, output)


if __name__ == "__main__":
    main()
